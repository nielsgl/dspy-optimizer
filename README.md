# DSPy Prompt-Optimizer Framework

A flexible framework that **iteratively improves prompts**-via self-critique and self-repairâ€”until a large-language model (LLM) achieves the desired output on a labelled dataset.
Although the reference implementation targets â€œamount-excluding-taxâ€ extraction from Dutch invoices, the design is **domain-agnostic**: swap in a new *signature* (I/O schema) and a *base prompt*, and the optimiser will refine prompts for *any* taskâ€”information extraction, classification, transformation, or reasoning.

---

## 1.â€¯Why this project exists

| Problem                                                     | Traditional fix                               | Limitations                                       | This frameworkâ€™s answer                                                                                              |
| ----------------------------------------------------------- | --------------------------------------------- | ------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- |
| Prompt engineering is manual, brittle, expensive.           | Humans iterate on prompts by trial-and-error. | Slow, non-repeatable, hard to audit.              | **Automatic prompt search** guided by test data and LLM self-reflection.                                             |
| Each business domain needs slightly different instructions. | Maintain ad-hoc prompt variants.              | Prompt zoo quickly diverges; hard to reuse ideas. | **Block-based prompt merging** collects successful heuristics/examples into one canonical prompt.                    |
| Prompts can regress when new edge-cases show up.            | Manual regression testing.                    | Easy to forget a hidden corner case.              | **Built-in regression loop** reruns frozen validation set after every merge; unsafe patches roll back automatically. |

---

## 2.â€¯Key ideas

1. **Evaluator â†’ Refiner â†’ Merger loop**
   *Evaluator* runs the current prompt; if output â‰  gold, *Refiner* asks the LLM to propose a patch that would fix it; *Merger* folds that patch into the universal prompt.
   Loops â‰¤â€¯*k* times per sample to avoid infinite churn.
2. **Self-critique with history**
   Each refinement step receives **all previous failed prompts** so the LLM does not repeat old mistakes.
3. **Numerical tolerance / custom scorers**
   Scoring is pluggable: exact-match, fuzzy string, semantic similarity, numeric Â±Ïµ, etc.
4. **Block-based prompts**
   Prompt text is split into `### Task`, `### Output format`, `### Examples`, `### Heuristics`.
   The merger appends new examples or heuristics to the correct blockâ€”prompt stays readable and under token limits.
5. **Parallel evaluation, serial merging**
   Invoices can be processed in parallel threads, but merging is serial to avoid merge conflicts.
6. **Audit trail**
   Every prompt, decision, score and timestamp can be logged to SQLite/W\&B so you can answer â€œ*Why did the prompt change on 2025-07-14â€¯15:03?*â€

---

## 3.â€¯Architecture at a glance

```text
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     wrong?    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  prompt patch  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Evaluator  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Refiner    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–ºâ”‚    Merger   â”‚
â”‚  (LLM run)  â”‚ yes           â”‚  (LLM self-   â”‚                â”‚   (diff /   â”‚
â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚  critique)    â”‚                â”‚ block merge)â”‚
      â”‚ no                    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   updated prompt
```

---

## 4.â€¯Directory layout

```
dspy_optimizer/
â”œâ”€â”€ invoice_amount_optimizer.py      # Reference implementation
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ optimizer.py                 # Generic Optimiser class
â”‚   â”œâ”€â”€ evaluator.py                 # Evaluator module
â”‚   â”œâ”€â”€ refiner.py                   # Refiner module
â”‚   â”œâ”€â”€ merger.py                    # Merger module
â”‚   â””â”€â”€ scoring.py                   # Pluggable scorers
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ dutch_invoices/              # Example dataset & scripts
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ develop.ipynb                # Example notebooks for development
â”œâ”€â”€ data/                            # Example datasets
â”‚   â””â”€â”€ invoices/                    # Example invoices dataset
â”œâ”€â”€ tests/                           # Unit & integration tests
â””â”€â”€ README.md                        # â† you are here
```

---

## 5.â€¯Coding rules & standards

| Area                      | Standard                                                                                        |
| ------------------------- | ----------------------------------------------------------------------------------------------- |
| **Python version**        | â‰¥â€¯3.12 (pattern-matching, `typing` improvements).                                               |
| **Style**                 | PEPâ€¯8 + *black* (100â€¯char line length) + *isort*.                                               |
| **Type hints**            | Mandatory in all public functions/classes. Use `mypy --strict`.                                 |
| **Docstrings**            | Google style. Public APIs must include *Args*, *Returns*, *Raises*, *Example*.                  |
| **Naming**                | `PascalCase` for classes, `snake_case` for functions/vars, ALL\_CAPS for constants.             |
| **Immutability**          | Prefer `dataclass(frozen=True)` for config objects.                                             |
| **Dependency management** | Use **uv**; keep `pyproject.toml` single-source of truth.                                   |
| **Logging**               | Standard `logging` lib. No `print` except in CLI demos. Use structured JSON if writing to file. |
| **Testing**               | **pytest** with 90â€¯% line coverage target. Mock LLM calls using fixtures.                       |
| **CI**                    | GitHub Actions: lint â†’ type-check â†’ tests â†’ build wheel.                                        |
| **Versioning**            | SemVer.                                                                                         |
| **Security**              | Secrets via env vars; *never* hard-code keys in repo.                                           |
| **Docs**                  | Use MkDocs Material; auto-generate API docs with `mkdocstrings`.                                |

---

## 6.â€¯How to port to **any** extraction/classification task

1. **Define a new DSPy `Signature`** describing your *input fields* and *output fields*.

   ```python
   class PhoneExtractor(dspy.Signature):
       file: Attachments = dspy.InputField()
       phone_number: str = dspy.OutputField()
   ```
2. **Write a seed prompt** inside a base class (or via cfg).

   ```python
   base_prompt = """
   ### Task
   Extract the Dutch phone number of the billing contact from the document...
   """
   ```
3. **Plug both into the generic `Optimiser`**.

   ```python
   opt = GenericPromptOptimiser(model, base_prompt, PhoneExtractor)
   opt.optimise(dataset)
   ```

The optimiser will reuse the same evaluator/refiner/merger logic; only the I/O schema and scoring function change.

---



### Happy automating your prompt engineeringÂ ğŸš€
